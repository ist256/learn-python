{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code to login to https://okpy.org/ and setup the assignment for submission\n",
    "from ist256 import okclient\n",
    "ok = okclient.Lab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Coding Lab: Web Services and APIs\n",
    "\n",
    "### Overview\n",
    "\n",
    "The web has long evolved from user-consumption to device consumption. In the early days of the web when you wanted to check the weather, you opened up your browser and visited a website. Nowadays your smart watch / smart phone retrieves the weather for you and displays it on the device. Your device can't predict the weather. It's simply consuming a weather based service. \n",
    "\n",
    "The key to making device consumption work are API's (Application Program Interfaces). Products we use everyday like smartphones, Amazon's Alexa, and gaming consoles all rely on API's. They seem \"smart\" and \"powerful\" but in actuality they're only interfacing with smart and powerful services in the cloud.\n",
    "\n",
    "API consumption is the new reality of programming; it is why we cover it in this course. Once you undersand how to conusme API's you can write a program to do almost anything and harness the power of the internet to make your own programs look \"smart\" and \"powerful.\" \n",
    "\n",
    "This lab will be a walk-through for how to use a Web API. Specifically we will use the Microsoft Azure Text Analytics API to features like sentiment and entity recognition to our programs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to make sure you have the pre-requisites!\n",
    "!pip install -q requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by importing the modules we will need\n",
    "import requests\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Configure the Azure Text Analytics API\n",
    "\n",
    "### First, sign up for Azure for Students\n",
    "\n",
    "First you will need to sign up for Microsoft Azure for Students. This is free for all Syracuse University Students. Azure is a cloud provider from Microsoft. \n",
    "\n",
    "1. Go to https://azure.microsoft.com/en-us/free/students/ \n",
    "2. Click Activate Now\n",
    "2. Login with your SU email `netid@syr.edu`\n",
    "3. Use your NetID Password.\n",
    "4. When you log-in it should take you to the Azure Portal https://portal.azure.com\n",
    "\n",
    "### Next Add Text Analytics\n",
    "\n",
    "From inside the Azure portal:\n",
    "\n",
    "1. Click **Create a Resource**\n",
    "2. Choose **Text Analytics** and select Create\n",
    "3. Fill out the form:\n",
    "   - Name: `ist256-text-analytics`\n",
    "   - Subscription: `Azure for Students`\n",
    "   - Location: `(US) West US`\n",
    "   - Pricing Tier: `F0` (Important: Select the free tier!)\n",
    "   - Resource Group: `ist256-yournetid` (You might have to create it first replace `yournetid` with your actual netid!)\n",
    "4. Click create.\n",
    "5. When the deployment is done, click **Go To Resource**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are now on the quickstart screen \n",
    "\n",
    "Record your Key and Endpoint here:\n",
    "\n",
    "Key1:   \n",
    "\n",
    "Endpoint:   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record these values in code, too\n",
    "key = \"key-here\"\n",
    "endpoint = \"endpoint-url-here\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Out your API\n",
    "\n",
    "The documentation for the API can be found here: https://westus.dev.cognitive.microsoft.com/docs/services/TextAnalytics-v3-0-Preview-1 \n",
    "\n",
    "Let's give it a try by using named entity recognition. This attempts to extract meaning from text and is quite useful in applications which require natual language understanding.  \n",
    "\n",
    "For all requests, you provide a list of documents you wish to enact on. In this case we will extract entities from the following phrases:\n",
    "\n",
    "1. \"I would not pay $5 to see that Star Wars movie next week.\"\n",
    "2. \"Syracuse and Rochester are nicer cities than Buffalo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'{endpoint}text/analytics/v3.0-preview.1/entities/recognition/general'\n",
    "header = { 'Ocp-Apim-Subscription-Key' : key}\n",
    "documents = {\"documents\": [\n",
    "            {\"id\": \"1\", \"text\": \"I would not pay $5 to see that Star Wars movie next week.\" },\n",
    "            {\"id\": \"2\", \"text\": \"Syracuse and Rochester are nicer cities than Buffalo.\" }\n",
    "         ]\n",
    "    }\n",
    "response = requests.post(url,headers=header, json=documents)\n",
    "entities = response.json()\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The response \n",
    "\n",
    "Notice the service does a nice job recogizing `$5` as a quantity and `next week` as a date range.\n",
    "\n",
    "It also figures out that Syracuse, Rochester and Buffalo are all locations. \n",
    "\n",
    "For each recoginzed entity, you are provided with a `score` (confidence score between 0-1), a type, and sub-type as appropriate. \n",
    "\n",
    "### Now You Try it!\n",
    "\n",
    "Re-write the example above to perform named entity extraction on the following text:\n",
    "\n",
    "`Four out of five New York City coders prefer Google to Microsoft.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write code here\n",
    "url = f'{endpoint}text/analytics/v3.0-preview.1/entities/recognition/general'\n",
    "header = { 'Ocp-Apim-Subscription-Key' : key}\n",
    "documents = {\"documents\": [\n",
    "            {\"id\": \"1\", \"text\": \"Four out of five New York City coders prefer Google to Microsoft.\" }\n",
    "        ]\n",
    "    }\n",
    "response = requests.post(url,headers=header, json=documents)\n",
    "entities = response.json()\n",
    "entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curious as to what you can detect?\n",
    "\n",
    "Check out: https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking#supported-types-for-named-entity-recognition-v2 \n",
    "\n",
    "Let's use this service write our own user-defined function to extract email addresses from text. \n",
    "\n",
    "The API call is the same, the difference is what we do with the results. We should loop through the entities and if the type is an email address, print it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"As of this year, my primary email address is mafudge@syr.edu but I also use mafudge@gmail.com and snookybear4182@aol.com from time to time.\"\n",
    "\n",
    "url = f'{endpoint}text/analytics/v3.0-preview.1/entities/recognition/general'\n",
    "header = { 'Ocp-Apim-Subscription-Key' : key}\n",
    "documents = {\"documents\": [\n",
    "            {\"id\": \"1\", \"text\": text }\n",
    "        ]\n",
    "    }\n",
    "response = requests.post(url,headers=header, json=documents)\n",
    "entities = response.json()\n",
    "for entity in entities['documents'][0]['entities']:\n",
    "    if entity['type'] == 'Email':\n",
    "        print( entity['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that you know how to use it, what can you do with it?\n",
    "\n",
    "Text Analytics technologies such as named entity recoginition, key phrase extraction and sentiment analysis are best used when combined with another service. For example:\n",
    "\n",
    "1. You can take a list of customer reviews from a diner restaurant and run sentiment analysis to determine how customers feel about it. Do they like this place or not?\n",
    "2. Use named entity recognition to determine where those customers are from or when they visited.\n",
    "3. Use key phrase extraction to determine what they are talking about? Pancakes, breakfast sandwiches, eggs, etc...\n",
    "\n",
    "### Now you try it \n",
    "\n",
    "Write a program to extract key phrases from the three reviews provided. Make 1 api call to the url endpoint have been provided for you. It's up to you to print out the key phrases!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = \"I don't think I will ever order the eggs again. Not very good.\"\n",
    "review2 = \"Went there last Wednesday. It was croweded and the pancakes and eggs were spot on! I enjoyed my meal.\"\n",
    "review3 = \"Not sure who is running the place but the eggs benedict were not that great. On the other hand I was happy with my toast.\"\n",
    "url = f'{endpoint}text/analytics/v3.0-preview.1/keyphrases'\n",
    "\n",
    "# TODO Write your code here\n",
    "header = { 'Ocp-Apim-Subscription-Key' : key}\n",
    "documents = {\"documents\": [\n",
    "            {\"id\": \"1\", \"text\": review1 },\n",
    "            {\"id\": \"2\", \"text\": review2 },\n",
    "            {\"id\": \"3\", \"text\": review3 }\n",
    "        ]\n",
    "    }\n",
    "response = requests.post(url,headers=header, json=documents)\n",
    "phrases = response.json()\n",
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrases is to Sentiment as Peanutbutter is to Jelly!\n",
    "\n",
    "From our key phrases analysis, it looks like customers are talking about `eggs` but are they speaking positively or negatively about their egg experiences? This is why sentiment analysis accompanies key phrase analysis. Key phrase identifies what they are talking about, and sentiment provides the context around it.\n",
    "\n",
    "### Now you try it\n",
    "\n",
    "Perform sentiment analysis over the reviews to determine who likes the eggs and who does not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = \"I don't think I will ever order the eggs again. Not very good.\"\n",
    "review2 = \"Went there last Wednesday. It was croweded and the pancakes and eggs were spot on! I enjoyed my meal.\"\n",
    "review3 = \"Not sure who is running the place but the eggs benedict were not that great. On the other hand I was happy with my toast.\"\n",
    "url = f'{endpoint}text/analytics/v3.0-preview.1/sentiment'\n",
    "\n",
    "# TODO: Write code here\n",
    "header = { 'Ocp-Apim-Subscription-Key' : key}\n",
    "documents = {\"documents\": [\n",
    "            {\"id\": \"1\", \"text\": review1 },\n",
    "            {\"id\": \"2\", \"text\": review2 },\n",
    "            {\"id\": \"3\", \"text\": review3 }\n",
    "        ]\n",
    "    }\n",
    "response = requests.post(url,headers=header, json=documents)\n",
    "sentiment = response.json()\n",
    "sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metacognition\n",
    "\n",
    "Please answer the following questions. This should be a personal narrative, in your own voice. Answer the questions by double clicking on the question and placing your answer next to the Answer: prompt.\n",
    "\n",
    "\n",
    "1. Record any questions you have about this lab that you would like to ask in recitation. It is expected you will have questions if you did not complete the code sections correctly.  Learning how to articulate what you do not understand is an important skill of critical thinking. \n",
    "\n",
    "Answer: \n",
    "\n",
    "2. What was the most difficult aspect of completing this lab? Least difficult?  \n",
    "\n",
    "Answer: \n",
    "\n",
    "3. What aspects of this lab do you find most valuable? Least valuable?\n",
    "\n",
    "Answer: \n",
    "\n",
    "4. Rate your comfort level with this week's material so far.   \n",
    "\n",
    "1 ==> I can do this on my own and explain how to do it.   \n",
    "2 ==> I can do this on my own without any help.   \n",
    "3 ==> I can do this with help or guidance from others. If you choose this level please list those who helped you.   \n",
    "4 ==> I don't understand this at all yet and need extra help. If you choose this please try to articulate that which you do not understand.   \n",
    "\n",
    "Answer: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save and turn in your work, execute this cell. Your latest submission will be graded. \n",
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
